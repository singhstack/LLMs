# LLMs

This repository covers building, training and evaluating large language models. We start from simple n-gram models and escalate to RNNs, attention based models, transformers.

## n-gram Models
In the first few commits, we covered n-gram models which use text corpus to build Probability Distribution Functions (PDF). These PDFs are used to predict the next word based on the sequence provided to the model. These sequences vary as 'n' in the n-gram model changes.

During the testing and evaluation phase, we compare results of uni-gram, bi-gram and tri-gram models


